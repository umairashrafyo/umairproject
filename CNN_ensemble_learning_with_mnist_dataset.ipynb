{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " CNN ensemble learning with mnist dataset.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1VcqamyTD8Hy4CKvBmUT0qOFFzeOcpy9y",
      "authorship_tag": "ABX9TyPprWNEmFLXb40TG9SYN2BC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/umairashrafyo/umairproject/blob/main/CNN_ensemble_learning_with_mnist_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2lQrZ5E82w2"
      },
      "source": [
        "#First, we need to import the required libraries. \n",
        "import tensorflow.keras\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, mean_squared_error\n",
        "from sklearn.model_selection import train_test_split\n",
        "import itertools\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
        "from keras.utils.np_utils import to_categorical # convert to one-hot-encoding\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.optimizers import Adam, RMSprop, Adagrad"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QaOIR38_9Jqv"
      },
      "source": [
        "#After importing the required libraries, we will read the MNIST handwritten data set that is provided publicly in Google Colab as sample data.\n",
        "#Reading the data\n",
        "train = pd.read_csv(\"/content/drive/MyDrive/mnist dataset/train.csv\")\n",
        "test = pd.read_csv(\"/content/drive/MyDrive/mnist dataset/test.csv\")"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "OdGxoO-19p85",
        "outputId": "b257f0c2-bc03-4e07-b32d-3f84ead0f9be"
      },
      "source": [
        "#Now, we need to specify the training and test sets. It will be done using the below lines of codes. First, we will check the header and then we will identify the required columns\n",
        "#Training data head\n",
        "train.head()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>pixel0</th>\n",
              "      <th>pixel1</th>\n",
              "      <th>pixel2</th>\n",
              "      <th>pixel3</th>\n",
              "      <th>pixel4</th>\n",
              "      <th>pixel5</th>\n",
              "      <th>pixel6</th>\n",
              "      <th>pixel7</th>\n",
              "      <th>pixel8</th>\n",
              "      <th>pixel9</th>\n",
              "      <th>pixel10</th>\n",
              "      <th>pixel11</th>\n",
              "      <th>pixel12</th>\n",
              "      <th>pixel13</th>\n",
              "      <th>pixel14</th>\n",
              "      <th>pixel15</th>\n",
              "      <th>pixel16</th>\n",
              "      <th>pixel17</th>\n",
              "      <th>pixel18</th>\n",
              "      <th>pixel19</th>\n",
              "      <th>pixel20</th>\n",
              "      <th>pixel21</th>\n",
              "      <th>pixel22</th>\n",
              "      <th>pixel23</th>\n",
              "      <th>pixel24</th>\n",
              "      <th>pixel25</th>\n",
              "      <th>pixel26</th>\n",
              "      <th>pixel27</th>\n",
              "      <th>pixel28</th>\n",
              "      <th>pixel29</th>\n",
              "      <th>pixel30</th>\n",
              "      <th>pixel31</th>\n",
              "      <th>pixel32</th>\n",
              "      <th>pixel33</th>\n",
              "      <th>pixel34</th>\n",
              "      <th>pixel35</th>\n",
              "      <th>pixel36</th>\n",
              "      <th>pixel37</th>\n",
              "      <th>pixel38</th>\n",
              "      <th>...</th>\n",
              "      <th>pixel744</th>\n",
              "      <th>pixel745</th>\n",
              "      <th>pixel746</th>\n",
              "      <th>pixel747</th>\n",
              "      <th>pixel748</th>\n",
              "      <th>pixel749</th>\n",
              "      <th>pixel750</th>\n",
              "      <th>pixel751</th>\n",
              "      <th>pixel752</th>\n",
              "      <th>pixel753</th>\n",
              "      <th>pixel754</th>\n",
              "      <th>pixel755</th>\n",
              "      <th>pixel756</th>\n",
              "      <th>pixel757</th>\n",
              "      <th>pixel758</th>\n",
              "      <th>pixel759</th>\n",
              "      <th>pixel760</th>\n",
              "      <th>pixel761</th>\n",
              "      <th>pixel762</th>\n",
              "      <th>pixel763</th>\n",
              "      <th>pixel764</th>\n",
              "      <th>pixel765</th>\n",
              "      <th>pixel766</th>\n",
              "      <th>pixel767</th>\n",
              "      <th>pixel768</th>\n",
              "      <th>pixel769</th>\n",
              "      <th>pixel770</th>\n",
              "      <th>pixel771</th>\n",
              "      <th>pixel772</th>\n",
              "      <th>pixel773</th>\n",
              "      <th>pixel774</th>\n",
              "      <th>pixel775</th>\n",
              "      <th>pixel776</th>\n",
              "      <th>pixel777</th>\n",
              "      <th>pixel778</th>\n",
              "      <th>pixel779</th>\n",
              "      <th>pixel780</th>\n",
              "      <th>pixel781</th>\n",
              "      <th>pixel782</th>\n",
              "      <th>pixel783</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 785 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   label  pixel0  pixel1  pixel2  ...  pixel780  pixel781  pixel782  pixel783\n",
              "0      1       0       0       0  ...         0         0         0         0\n",
              "1      0       0       0       0  ...         0         0         0         0\n",
              "2      1       0       0       0  ...         0         0         0         0\n",
              "3      4       0       0       0  ...         0         0         0         0\n",
              "4      0       0       0       0  ...         0         0         0         0\n",
              "\n",
              "[5 rows x 785 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRZglnZO9uaq",
        "outputId": "20d32549-8800-4384-9ca7-ba24a7287815"
      },
      "source": [
        "#Specifying train and test data\n",
        "train_X = train.iloc[:,1:]\n",
        "train_y = train.iloc[:,0]\n",
        "test = test.iloc[:,1:]\n",
        "\n",
        "#Shape of the specified data\n",
        "print(train_X.shape)\n",
        "print(train_y.shape)\n",
        "print(test.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(19999, 784)\n",
            "(19999,)\n",
            "(9999, 784)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82S6BIR_9wrL"
      },
      "source": [
        "#Now, we will normalize the training and test data\n",
        "#Normalize the data\n",
        "train_X = train_X / 255.0\n",
        "test = test / 255.0"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5xfa0BdJ9zCU"
      },
      "source": [
        "#Reshape image in 3 dimensions (with 1 channel)\n",
        "train_X = train_X.values.reshape(-1,28,28,1)\n",
        "test = test.values.reshape(-1,28,28,1)\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b2xOSDa491Qm"
      },
      "source": [
        "#Encode labels to one hot vectors\n",
        "train_y = to_categorical(train_y, num_classes = 10)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "n8SpemK693kn",
        "outputId": "1ecb25f3-08e6-4ee2-cf1b-0d2e7ee8aa10"
      },
      "source": [
        "#Sample image\n",
        "plt.imshow(train_X[0][:,:,0])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f800c348fd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dWpLD9RY96iX",
        "outputId": "97c2bbc7-d7af-4ca7-e74e-147a3d041c1b"
      },
      "source": [
        "#Ensemble Of Convolutional Neural Networks\n",
        "# Define 10 CNN models\n",
        "from keras.optimizers import RMSprop, Adam\n",
        "from keras.layers import DepthwiseConv2D, Reshape, Activation\n",
        "\n",
        "nets = 10\n",
        "model = [0] *nets\n",
        "\n",
        "for j in range(nets):\n",
        "    model[j] = Sequential()\n",
        "    #First Layer\n",
        "    model[j].add(Conv2D(32, kernel_size = 3, activation='relu', input_shape = (28, 28, 1)))\n",
        "    model[j].add(BatchNormalization())\n",
        "    model[j].add(Conv2D(32, kernel_size = 3, activation='relu'))\n",
        "    model[j].add(BatchNormalization())\n",
        "    model[j].add(Conv2D(32, kernel_size = 5, strides=2, padding='same', activation='relu'))\n",
        "    model[j].add(BatchNormalization())\n",
        "    model[j].add(Dropout(0.4))\n",
        "\n",
        "    #Second Layer\n",
        "    model[j].add(Conv2D(64, kernel_size = 3, activation='relu'))\n",
        "    model[j].add(BatchNormalization())\n",
        "    model[j].add(Conv2D(64, kernel_size = 3, activation='relu'))\n",
        "    model[j].add(BatchNormalization())\n",
        "    model[j].add(Conv2D(64, kernel_size = 5, strides=2, padding='same', activation='relu'))\n",
        "    model[j].add(BatchNormalization())\n",
        "    model[j].add(Dropout(0.4))\n",
        "\n",
        "    #Third layer\n",
        "    model[j].add(Conv2D(128, kernel_size = 4, activation='relu'))\n",
        "    model[j].add(BatchNormalization())\n",
        "    model[j].add(Flatten())\n",
        "    model[j].add(Dropout(0.4))\n",
        "\n",
        "    #Output layer\n",
        "    model[j].add(Dense(10, activation='softmax'))\n",
        "\n",
        "    # Compile each model\n",
        "    model[j].compile(optimizer='adam', loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "    \n",
        "print('All Models Defined')\n"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "All Models Defined\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yyfv1a0R-Hux"
      },
      "source": [
        "#We will use the learning rate annealer in this experiment. The learning rate annealer decreases the learning rate after a certain number of epochs if the error rate does not change. Here, through this technique, we will monitor the validation accuracy and if it seems to be a plateau in 3 epochs, it will reduce the learning rate.\n",
        "#LR Reduction Callback\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "learning_rate_reduction=ReduceLROnPlateau(monitor='val_accuracy', patience=3, verbose=0, factor=0.5, min_lr=0.00001)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gYb1Py1e-KoU",
        "outputId": "6c45ad6d-0c98-4416-b9ca-bbb9c432dffa"
      },
      "source": [
        "#In the next step, we will train the models that we have defined above.\n",
        "# train for 20 epochs\n",
        "# train for 20 epochs\n",
        "history = [0] * nets\n",
        "epochs = 20\n",
        "\n",
        "datagen = ImageDataGenerator(rotation_range=13, zoom_range=0.11, width_shift_range=0.1, height_shift_range=0.1)\n",
        "\n",
        "datagen.fit(train_X)\n",
        "\n",
        "for j in range(nets):\n",
        "    print(f'Individual Net : {j+1}')   \n",
        "    X_train2, X_val2, Y_train2, Y_val2 = train_test_split(train_X, train_y, test_size = 0.1)\n",
        "    history[j] = model[j].fit(datagen.flow(X_train2,Y_train2, batch_size=64), epochs = epochs, steps_per_epoch = X_train2.shape[0]//64, validation_data = (X_val2,Y_val2), callbacks=[learning_rate_reduction], verbose=1)\n",
        "\n",
        "    print(\"CNN Model {0:d}: Epochs={1:d}, Training accuracy={2:.5f}, Validation accuracy={3:.5f}\".format(j+1,epochs,max(history[j].history['accuracy']),max(history[j].history['val_accuracy']) ))\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Individual Net : 1\n",
            "Epoch 1/20\n",
            "281/281 [==============================] - 80s 279ms/step - loss: 1.4529 - accuracy: 0.5668 - val_loss: 7.0598 - val_accuracy: 0.1025\n",
            "Epoch 2/20\n",
            "281/281 [==============================] - 78s 278ms/step - loss: 0.2601 - accuracy: 0.9183 - val_loss: 0.3419 - val_accuracy: 0.8820\n",
            "Epoch 3/20\n",
            "281/281 [==============================] - 79s 280ms/step - loss: 0.1526 - accuracy: 0.9535 - val_loss: 0.0444 - val_accuracy: 0.9880\n",
            "Epoch 4/20\n",
            "281/281 [==============================] - 79s 281ms/step - loss: 0.1354 - accuracy: 0.9562 - val_loss: 0.0443 - val_accuracy: 0.9865\n",
            "Epoch 5/20\n",
            "281/281 [==============================] - 78s 279ms/step - loss: 0.1121 - accuracy: 0.9655 - val_loss: 0.0653 - val_accuracy: 0.9840\n",
            "Epoch 6/20\n",
            "281/281 [==============================] - 78s 279ms/step - loss: 0.1003 - accuracy: 0.9683 - val_loss: 0.0313 - val_accuracy: 0.9910\n",
            "Epoch 7/20\n",
            "281/281 [==============================] - 78s 278ms/step - loss: 0.0935 - accuracy: 0.9746 - val_loss: 0.0378 - val_accuracy: 0.9905\n",
            "Epoch 8/20\n",
            "281/281 [==============================] - 78s 277ms/step - loss: 0.0829 - accuracy: 0.9743 - val_loss: 0.0531 - val_accuracy: 0.9845\n",
            "Epoch 9/20\n",
            "281/281 [==============================] - 78s 276ms/step - loss: 0.0806 - accuracy: 0.9750 - val_loss: 0.0404 - val_accuracy: 0.9900\n",
            "Epoch 10/20\n",
            "281/281 [==============================] - 78s 276ms/step - loss: 0.0737 - accuracy: 0.9771 - val_loss: 0.0244 - val_accuracy: 0.9930\n",
            "Epoch 11/20\n",
            "281/281 [==============================] - 78s 277ms/step - loss: 0.0599 - accuracy: 0.9816 - val_loss: 0.0308 - val_accuracy: 0.9895\n",
            "Epoch 12/20\n",
            "281/281 [==============================] - 77s 274ms/step - loss: 0.0556 - accuracy: 0.9837 - val_loss: 0.0293 - val_accuracy: 0.9920\n",
            "Epoch 13/20\n",
            "281/281 [==============================] - 77s 275ms/step - loss: 0.0551 - accuracy: 0.9844 - val_loss: 0.0215 - val_accuracy: 0.9920\n",
            "Epoch 14/20\n",
            "281/281 [==============================] - 77s 275ms/step - loss: 0.0449 - accuracy: 0.9870 - val_loss: 0.0239 - val_accuracy: 0.9935\n",
            "Epoch 15/20\n",
            "281/281 [==============================] - 78s 277ms/step - loss: 0.0414 - accuracy: 0.9874 - val_loss: 0.0208 - val_accuracy: 0.9935\n",
            "Epoch 16/20\n",
            "281/281 [==============================] - 78s 276ms/step - loss: 0.0412 - accuracy: 0.9878 - val_loss: 0.0230 - val_accuracy: 0.9915\n",
            "Epoch 17/20\n",
            "281/281 [==============================] - 77s 274ms/step - loss: 0.0459 - accuracy: 0.9857 - val_loss: 0.0224 - val_accuracy: 0.9925\n",
            "Epoch 18/20\n",
            "281/281 [==============================] - 78s 276ms/step - loss: 0.0386 - accuracy: 0.9866 - val_loss: 0.0187 - val_accuracy: 0.9925\n",
            "Epoch 19/20\n",
            "281/281 [==============================] - 77s 275ms/step - loss: 0.0353 - accuracy: 0.9897 - val_loss: 0.0219 - val_accuracy: 0.9945\n",
            "Epoch 20/20\n",
            "281/281 [==============================] - 78s 276ms/step - loss: 0.0320 - accuracy: 0.9893 - val_loss: 0.0267 - val_accuracy: 0.9925\n",
            "CNN Model 1: Epochs=20, Training accuracy=0.98968, Validation accuracy=0.99450\n",
            "Individual Net : 2\n",
            "Epoch 1/20\n",
            "281/281 [==============================] - 79s 276ms/step - loss: 1.4379 - accuracy: 0.5727 - val_loss: 6.3697 - val_accuracy: 0.1080\n",
            "Epoch 2/20\n",
            "281/281 [==============================] - 78s 277ms/step - loss: 0.2562 - accuracy: 0.9206 - val_loss: 0.2527 - val_accuracy: 0.9200\n",
            "Epoch 3/20\n",
            "281/281 [==============================] - 78s 277ms/step - loss: 0.1790 - accuracy: 0.9424 - val_loss: 0.0587 - val_accuracy: 0.9815\n",
            "Epoch 4/20\n",
            "281/281 [==============================] - 77s 274ms/step - loss: 0.1486 - accuracy: 0.9553 - val_loss: 0.0702 - val_accuracy: 0.9825\n",
            "Epoch 5/20\n",
            "281/281 [==============================] - 77s 273ms/step - loss: 0.1267 - accuracy: 0.9628 - val_loss: 0.0988 - val_accuracy: 0.9760\n",
            "Epoch 6/20\n",
            "281/281 [==============================] - 77s 273ms/step - loss: 0.1168 - accuracy: 0.9615 - val_loss: 0.0555 - val_accuracy: 0.9815\n",
            "Epoch 7/20\n",
            "281/281 [==============================] - 77s 274ms/step - loss: 0.0962 - accuracy: 0.9701 - val_loss: 0.0488 - val_accuracy: 0.9840\n",
            "Epoch 8/20\n",
            "281/281 [==============================] - 77s 274ms/step - loss: 0.0923 - accuracy: 0.9724 - val_loss: 0.0581 - val_accuracy: 0.9830\n",
            "Epoch 9/20\n",
            "281/281 [==============================] - 77s 272ms/step - loss: 0.0819 - accuracy: 0.9763 - val_loss: 0.0355 - val_accuracy: 0.9855\n",
            "Epoch 10/20\n",
            "281/281 [==============================] - 77s 273ms/step - loss: 0.0793 - accuracy: 0.9771 - val_loss: 0.0341 - val_accuracy: 0.9885\n",
            "Epoch 11/20\n",
            "281/281 [==============================] - 77s 274ms/step - loss: 0.0850 - accuracy: 0.9740 - val_loss: 0.0389 - val_accuracy: 0.9875\n",
            "Epoch 12/20\n",
            "281/281 [==============================] - 77s 273ms/step - loss: 0.0793 - accuracy: 0.9729 - val_loss: 0.0289 - val_accuracy: 0.9910\n",
            "Epoch 13/20\n",
            "281/281 [==============================] - 77s 274ms/step - loss: 0.0656 - accuracy: 0.9810 - val_loss: 0.0364 - val_accuracy: 0.9875\n",
            "Epoch 14/20\n",
            "281/281 [==============================] - 77s 274ms/step - loss: 0.0670 - accuracy: 0.9791 - val_loss: 0.0784 - val_accuracy: 0.9785\n",
            "Epoch 15/20\n",
            "281/281 [==============================] - 77s 274ms/step - loss: 0.0661 - accuracy: 0.9803 - val_loss: 0.0305 - val_accuracy: 0.9905\n",
            "Epoch 16/20\n",
            "281/281 [==============================] - 76s 272ms/step - loss: 0.0548 - accuracy: 0.9833 - val_loss: 0.0269 - val_accuracy: 0.9920\n",
            "Epoch 17/20\n",
            "281/281 [==============================] - 76s 272ms/step - loss: 0.0504 - accuracy: 0.9843 - val_loss: 0.0270 - val_accuracy: 0.9910\n",
            "Epoch 18/20\n",
            "281/281 [==============================] - 77s 273ms/step - loss: 0.0432 - accuracy: 0.9859 - val_loss: 0.0220 - val_accuracy: 0.9935\n",
            "Epoch 19/20\n",
            "281/281 [==============================] - 77s 275ms/step - loss: 0.0430 - accuracy: 0.9871 - val_loss: 0.0278 - val_accuracy: 0.9925\n",
            "Epoch 20/20\n",
            "281/281 [==============================] - 77s 273ms/step - loss: 0.0459 - accuracy: 0.9858 - val_loss: 0.0226 - val_accuracy: 0.9940\n",
            "CNN Model 2: Epochs=20, Training accuracy=0.98723, Validation accuracy=0.99400\n",
            "Individual Net : 3\n",
            "Epoch 1/20\n",
            "281/281 [==============================] - 78s 274ms/step - loss: 1.3768 - accuracy: 0.5957 - val_loss: 3.2986 - val_accuracy: 0.3585\n",
            "Epoch 2/20\n",
            "281/281 [==============================] - 77s 274ms/step - loss: 0.2248 - accuracy: 0.9323 - val_loss: 0.2539 - val_accuracy: 0.9210\n",
            "Epoch 3/20\n",
            "281/281 [==============================] - 77s 276ms/step - loss: 0.1646 - accuracy: 0.9520 - val_loss: 0.0474 - val_accuracy: 0.9880\n",
            "Epoch 4/20\n",
            "281/281 [==============================] - 77s 274ms/step - loss: 0.1254 - accuracy: 0.9636 - val_loss: 0.0423 - val_accuracy: 0.9860\n",
            "Epoch 5/20\n",
            "281/281 [==============================] - 77s 273ms/step - loss: 0.1135 - accuracy: 0.9664 - val_loss: 0.0413 - val_accuracy: 0.9890\n",
            "Epoch 6/20\n",
            "281/281 [==============================] - 77s 275ms/step - loss: 0.0925 - accuracy: 0.9723 - val_loss: 0.0368 - val_accuracy: 0.9885\n",
            "Epoch 7/20\n",
            "281/281 [==============================] - 78s 279ms/step - loss: 0.0948 - accuracy: 0.9717 - val_loss: 0.0392 - val_accuracy: 0.9910\n",
            "Epoch 8/20\n",
            "281/281 [==============================] - 78s 278ms/step - loss: 0.0887 - accuracy: 0.9729 - val_loss: 0.0459 - val_accuracy: 0.9840\n",
            "Epoch 9/20\n",
            "281/281 [==============================] - 78s 276ms/step - loss: 0.0789 - accuracy: 0.9753 - val_loss: 0.0373 - val_accuracy: 0.9895\n",
            "Epoch 10/20\n",
            "281/281 [==============================] - 77s 274ms/step - loss: 0.0776 - accuracy: 0.9755 - val_loss: 0.0332 - val_accuracy: 0.9885\n",
            "Epoch 11/20\n",
            "281/281 [==============================] - 77s 275ms/step - loss: 0.0580 - accuracy: 0.9822 - val_loss: 0.0243 - val_accuracy: 0.9925\n",
            "Epoch 12/20\n",
            "281/281 [==============================] - 77s 274ms/step - loss: 0.0429 - accuracy: 0.9871 - val_loss: 0.0236 - val_accuracy: 0.9935\n",
            "Epoch 13/20\n",
            "281/281 [==============================] - 77s 274ms/step - loss: 0.0450 - accuracy: 0.9854 - val_loss: 0.0251 - val_accuracy: 0.9930\n",
            "Epoch 14/20\n",
            "281/281 [==============================] - 78s 277ms/step - loss: 0.0501 - accuracy: 0.9840 - val_loss: 0.0249 - val_accuracy: 0.9925\n",
            "Epoch 15/20\n",
            "281/281 [==============================] - 77s 275ms/step - loss: 0.0475 - accuracy: 0.9864 - val_loss: 0.0261 - val_accuracy: 0.9935\n",
            "Epoch 16/20\n",
            "281/281 [==============================] - 77s 274ms/step - loss: 0.0421 - accuracy: 0.9867 - val_loss: 0.0174 - val_accuracy: 0.9950\n",
            "Epoch 17/20\n",
            "281/281 [==============================] - 77s 273ms/step - loss: 0.0391 - accuracy: 0.9889 - val_loss: 0.0202 - val_accuracy: 0.9940\n",
            "Epoch 18/20\n",
            "281/281 [==============================] - 77s 273ms/step - loss: 0.0361 - accuracy: 0.9888 - val_loss: 0.0228 - val_accuracy: 0.9925\n",
            "Epoch 19/20\n",
            "281/281 [==============================] - 77s 273ms/step - loss: 0.0395 - accuracy: 0.9882 - val_loss: 0.0189 - val_accuracy: 0.9930\n",
            "Epoch 20/20\n",
            "281/281 [==============================] - 77s 274ms/step - loss: 0.0340 - accuracy: 0.9891 - val_loss: 0.0183 - val_accuracy: 0.9935\n",
            "CNN Model 3: Epochs=20, Training accuracy=0.98913, Validation accuracy=0.99500\n",
            "Individual Net : 4\n",
            "Epoch 1/20\n",
            "281/281 [==============================] - 78s 275ms/step - loss: 1.4567 - accuracy: 0.5632 - val_loss: 2.5575 - val_accuracy: 0.1740\n",
            "Epoch 2/20\n",
            "281/281 [==============================] - 77s 273ms/step - loss: 0.2416 - accuracy: 0.9267 - val_loss: 0.0983 - val_accuracy: 0.9710\n",
            "Epoch 3/20\n",
            "281/281 [==============================] - 77s 274ms/step - loss: 0.1765 - accuracy: 0.9445 - val_loss: 0.0637 - val_accuracy: 0.9795\n",
            "Epoch 4/20\n",
            "281/281 [==============================] - 77s 273ms/step - loss: 0.1367 - accuracy: 0.9595 - val_loss: 0.0569 - val_accuracy: 0.9835\n",
            "Epoch 5/20\n",
            "281/281 [==============================] - 77s 275ms/step - loss: 0.0967 - accuracy: 0.9699 - val_loss: 0.0527 - val_accuracy: 0.9830\n",
            "Epoch 6/20\n",
            "281/281 [==============================] - 77s 274ms/step - loss: 0.1060 - accuracy: 0.9655 - val_loss: 0.0618 - val_accuracy: 0.9795\n",
            "Epoch 7/20\n",
            "281/281 [==============================] - 78s 277ms/step - loss: 0.1033 - accuracy: 0.9679 - val_loss: 0.0483 - val_accuracy: 0.9845\n",
            "Epoch 8/20\n",
            "281/281 [==============================] - 78s 276ms/step - loss: 0.0948 - accuracy: 0.9743 - val_loss: 0.0429 - val_accuracy: 0.9860\n",
            "Epoch 9/20\n",
            "281/281 [==============================] - 78s 278ms/step - loss: 0.0817 - accuracy: 0.9747 - val_loss: 0.0416 - val_accuracy: 0.9860\n",
            "Epoch 10/20\n",
            "281/281 [==============================] - 78s 276ms/step - loss: 0.0793 - accuracy: 0.9740 - val_loss: 0.0389 - val_accuracy: 0.9865\n",
            "Epoch 11/20\n",
            "281/281 [==============================] - 77s 274ms/step - loss: 0.0648 - accuracy: 0.9789 - val_loss: 0.0584 - val_accuracy: 0.9820\n",
            "Epoch 12/20\n",
            "281/281 [==============================] - 77s 275ms/step - loss: 0.0672 - accuracy: 0.9785 - val_loss: 0.0295 - val_accuracy: 0.9885\n",
            "Epoch 13/20\n",
            "281/281 [==============================] - 77s 276ms/step - loss: 0.0732 - accuracy: 0.9775 - val_loss: 0.0369 - val_accuracy: 0.9905\n",
            "Epoch 14/20\n",
            "281/281 [==============================] - 77s 275ms/step - loss: 0.0606 - accuracy: 0.9810 - val_loss: 0.0315 - val_accuracy: 0.9905\n",
            "Epoch 15/20\n",
            "281/281 [==============================] - 77s 275ms/step - loss: 0.0630 - accuracy: 0.9799 - val_loss: 0.0472 - val_accuracy: 0.9865\n",
            "Epoch 16/20\n",
            "281/281 [==============================] - 77s 274ms/step - loss: 0.0675 - accuracy: 0.9807 - val_loss: 0.0331 - val_accuracy: 0.9900\n",
            "Epoch 17/20\n",
            "281/281 [==============================] - 77s 275ms/step - loss: 0.0478 - accuracy: 0.9845 - val_loss: 0.0254 - val_accuracy: 0.9920\n",
            "Epoch 18/20\n",
            "281/281 [==============================] - 77s 273ms/step - loss: 0.0465 - accuracy: 0.9861 - val_loss: 0.0198 - val_accuracy: 0.9945\n",
            "Epoch 19/20\n",
            "281/281 [==============================] - 77s 274ms/step - loss: 0.0403 - accuracy: 0.9878 - val_loss: 0.0251 - val_accuracy: 0.9930\n",
            "Epoch 20/20\n",
            "281/281 [==============================] - 77s 275ms/step - loss: 0.0422 - accuracy: 0.9857 - val_loss: 0.0283 - val_accuracy: 0.9920\n",
            "CNN Model 4: Epochs=20, Training accuracy=0.98740, Validation accuracy=0.99450\n",
            "Individual Net : 5\n",
            "Epoch 1/20\n",
            "281/281 [==============================] - 78s 275ms/step - loss: 1.3132 - accuracy: 0.6032 - val_loss: 4.2416 - val_accuracy: 0.3360\n",
            "Epoch 2/20\n",
            "281/281 [==============================] - 77s 274ms/step - loss: 0.2491 - accuracy: 0.9224 - val_loss: 0.4828 - val_accuracy: 0.8635\n",
            "Epoch 3/20\n",
            "281/281 [==============================] - 77s 274ms/step - loss: 0.1674 - accuracy: 0.9453 - val_loss: 0.0587 - val_accuracy: 0.9815\n",
            "Epoch 4/20\n",
            "281/281 [==============================] - 77s 274ms/step - loss: 0.1218 - accuracy: 0.9632 - val_loss: 0.0680 - val_accuracy: 0.9785\n",
            "Epoch 5/20\n",
            "281/281 [==============================] - 77s 275ms/step - loss: 0.1082 - accuracy: 0.9688 - val_loss: 0.0413 - val_accuracy: 0.9875\n",
            "Epoch 6/20\n",
            "281/281 [==============================] - 78s 276ms/step - loss: 0.1119 - accuracy: 0.9665 - val_loss: 0.0325 - val_accuracy: 0.9905\n",
            "Epoch 7/20\n",
            "281/281 [==============================] - 77s 275ms/step - loss: 0.0885 - accuracy: 0.9737 - val_loss: 0.0417 - val_accuracy: 0.9865\n",
            "Epoch 8/20\n",
            "281/281 [==============================] - 77s 274ms/step - loss: 0.0854 - accuracy: 0.9749 - val_loss: 0.0361 - val_accuracy: 0.9865\n",
            "Epoch 9/20\n",
            "281/281 [==============================] - 77s 276ms/step - loss: 0.0855 - accuracy: 0.9744 - val_loss: 0.0398 - val_accuracy: 0.9890\n",
            "Epoch 10/20\n",
            "281/281 [==============================] - 78s 276ms/step - loss: 0.0662 - accuracy: 0.9809 - val_loss: 0.0317 - val_accuracy: 0.9890\n",
            "Epoch 11/20\n",
            "281/281 [==============================] - 77s 275ms/step - loss: 0.0597 - accuracy: 0.9821 - val_loss: 0.0277 - val_accuracy: 0.9905\n",
            "Epoch 12/20\n",
            "281/281 [==============================] - 77s 276ms/step - loss: 0.0585 - accuracy: 0.9823 - val_loss: 0.0283 - val_accuracy: 0.9915\n",
            "Epoch 13/20\n",
            "281/281 [==============================] - 78s 278ms/step - loss: 0.0539 - accuracy: 0.9839 - val_loss: 0.0250 - val_accuracy: 0.9915\n",
            "Epoch 14/20\n",
            "281/281 [==============================] - 79s 280ms/step - loss: 0.0646 - accuracy: 0.9803 - val_loss: 0.0382 - val_accuracy: 0.9895\n",
            "Epoch 15/20\n",
            "281/281 [==============================] - 79s 280ms/step - loss: 0.0512 - accuracy: 0.9852 - val_loss: 0.0313 - val_accuracy: 0.9890\n",
            "Epoch 16/20\n",
            "281/281 [==============================] - 79s 280ms/step - loss: 0.0488 - accuracy: 0.9852 - val_loss: 0.0263 - val_accuracy: 0.9930\n",
            "Epoch 17/20\n",
            "281/281 [==============================] - 79s 282ms/step - loss: 0.0412 - accuracy: 0.9880 - val_loss: 0.0231 - val_accuracy: 0.9930\n",
            "Epoch 18/20\n",
            "281/281 [==============================] - 78s 277ms/step - loss: 0.0396 - accuracy: 0.9882 - val_loss: 0.0276 - val_accuracy: 0.9925\n",
            "Epoch 19/20\n",
            "281/281 [==============================] - 78s 277ms/step - loss: 0.0413 - accuracy: 0.9872 - val_loss: 0.0214 - val_accuracy: 0.9950\n",
            "Epoch 20/20\n",
            "281/281 [==============================] - 78s 276ms/step - loss: 0.0354 - accuracy: 0.9888 - val_loss: 0.0235 - val_accuracy: 0.9955\n",
            "CNN Model 5: Epochs=20, Training accuracy=0.98827, Validation accuracy=0.99550\n",
            "Individual Net : 6\n",
            "Epoch 1/20\n",
            "281/281 [==============================] - 79s 277ms/step - loss: 1.4138 - accuracy: 0.5842 - val_loss: 6.7760 - val_accuracy: 0.1100\n",
            "Epoch 2/20\n",
            "281/281 [==============================] - 77s 276ms/step - loss: 0.2323 - accuracy: 0.9287 - val_loss: 0.0785 - val_accuracy: 0.9735\n",
            "Epoch 3/20\n",
            "281/281 [==============================] - 77s 275ms/step - loss: 0.1687 - accuracy: 0.9507 - val_loss: 0.0551 - val_accuracy: 0.9840\n",
            "Epoch 4/20\n",
            "281/281 [==============================] - 77s 276ms/step - loss: 0.1376 - accuracy: 0.9573 - val_loss: 0.0356 - val_accuracy: 0.9870\n",
            "Epoch 5/20\n",
            "281/281 [==============================] - 77s 275ms/step - loss: 0.1147 - accuracy: 0.9657 - val_loss: 0.0270 - val_accuracy: 0.9935\n",
            "Epoch 6/20\n",
            "281/281 [==============================] - 78s 276ms/step - loss: 0.0963 - accuracy: 0.9714 - val_loss: 0.0346 - val_accuracy: 0.9885\n",
            "Epoch 7/20\n",
            "281/281 [==============================] - 78s 276ms/step - loss: 0.1009 - accuracy: 0.9705 - val_loss: 0.0387 - val_accuracy: 0.9880\n",
            "Epoch 8/20\n",
            "281/281 [==============================] - 77s 275ms/step - loss: 0.0845 - accuracy: 0.9735 - val_loss: 0.0131 - val_accuracy: 0.9960\n",
            "Epoch 9/20\n",
            "281/281 [==============================] - 78s 276ms/step - loss: 0.0854 - accuracy: 0.9746 - val_loss: 0.0190 - val_accuracy: 0.9935\n",
            "Epoch 10/20\n",
            "281/281 [==============================] - 78s 276ms/step - loss: 0.0812 - accuracy: 0.9768 - val_loss: 0.0197 - val_accuracy: 0.9930\n",
            "Epoch 11/20\n",
            "281/281 [==============================] - 78s 277ms/step - loss: 0.0773 - accuracy: 0.9758 - val_loss: 0.0187 - val_accuracy: 0.9940\n",
            "Epoch 12/20\n",
            "281/281 [==============================] - 78s 276ms/step - loss: 0.0506 - accuracy: 0.9852 - val_loss: 0.0137 - val_accuracy: 0.9960\n",
            "Epoch 13/20\n",
            "281/281 [==============================] - 78s 276ms/step - loss: 0.0444 - accuracy: 0.9857 - val_loss: 0.0108 - val_accuracy: 0.9955\n",
            "Epoch 14/20\n",
            "281/281 [==============================] - 78s 276ms/step - loss: 0.0532 - accuracy: 0.9824 - val_loss: 0.0181 - val_accuracy: 0.9935\n",
            "Epoch 15/20\n",
            "281/281 [==============================] - 77s 276ms/step - loss: 0.0445 - accuracy: 0.9861 - val_loss: 0.0100 - val_accuracy: 0.9975\n",
            "Epoch 16/20\n",
            "281/281 [==============================] - 78s 279ms/step - loss: 0.0410 - accuracy: 0.9881 - val_loss: 0.0115 - val_accuracy: 0.9975\n",
            "Epoch 17/20\n",
            "281/281 [==============================] - 79s 279ms/step - loss: 0.0404 - accuracy: 0.9864 - val_loss: 0.0136 - val_accuracy: 0.9965\n",
            "Epoch 18/20\n",
            "281/281 [==============================] - 78s 277ms/step - loss: 0.0388 - accuracy: 0.9883 - val_loss: 0.0128 - val_accuracy: 0.9970\n",
            "Epoch 19/20\n",
            "281/281 [==============================] - 78s 277ms/step - loss: 0.0344 - accuracy: 0.9885 - val_loss: 0.0093 - val_accuracy: 0.9980\n",
            "Epoch 20/20\n",
            "281/281 [==============================] - 78s 277ms/step - loss: 0.0334 - accuracy: 0.9894 - val_loss: 0.0102 - val_accuracy: 0.9975\n",
            "CNN Model 6: Epochs=20, Training accuracy=0.98902, Validation accuracy=0.99800\n",
            "Individual Net : 7\n",
            "Epoch 1/20\n",
            "281/281 [==============================] - 79s 277ms/step - loss: 1.3972 - accuracy: 0.5715 - val_loss: 4.8938 - val_accuracy: 0.2420\n",
            "Epoch 2/20\n",
            "281/281 [==============================] - 78s 278ms/step - loss: 0.2617 - accuracy: 0.9184 - val_loss: 0.2262 - val_accuracy: 0.9245\n",
            "Epoch 3/20\n",
            "281/281 [==============================] - 78s 278ms/step - loss: 0.1828 - accuracy: 0.9471 - val_loss: 0.0800 - val_accuracy: 0.9770\n",
            "Epoch 4/20\n",
            "281/281 [==============================] - 78s 277ms/step - loss: 0.1475 - accuracy: 0.9540 - val_loss: 0.0525 - val_accuracy: 0.9830\n",
            "Epoch 5/20\n",
            "281/281 [==============================] - 78s 277ms/step - loss: 0.1207 - accuracy: 0.9646 - val_loss: 0.1239 - val_accuracy: 0.9625\n",
            "Epoch 6/20\n",
            "281/281 [==============================] - 78s 279ms/step - loss: 0.1058 - accuracy: 0.9667 - val_loss: 0.0315 - val_accuracy: 0.9895\n",
            "Epoch 7/20\n",
            "281/281 [==============================] - 78s 277ms/step - loss: 0.0887 - accuracy: 0.9728 - val_loss: 0.0301 - val_accuracy: 0.9885\n",
            "Epoch 8/20\n",
            "281/281 [==============================] - 78s 277ms/step - loss: 0.0864 - accuracy: 0.9746 - val_loss: 0.0759 - val_accuracy: 0.9755\n",
            "Epoch 9/20\n",
            "281/281 [==============================] - 78s 277ms/step - loss: 0.0906 - accuracy: 0.9733 - val_loss: 0.0224 - val_accuracy: 0.9925\n",
            "Epoch 10/20\n",
            "281/281 [==============================] - 78s 277ms/step - loss: 0.0690 - accuracy: 0.9780 - val_loss: 0.0266 - val_accuracy: 0.9930\n",
            "Epoch 11/20\n",
            "281/281 [==============================] - 78s 278ms/step - loss: 0.0686 - accuracy: 0.9792 - val_loss: 0.0332 - val_accuracy: 0.9905\n",
            "Epoch 12/20\n",
            "281/281 [==============================] - 78s 278ms/step - loss: 0.0750 - accuracy: 0.9761 - val_loss: 0.0355 - val_accuracy: 0.9880\n",
            "Epoch 13/20\n",
            "281/281 [==============================] - 78s 278ms/step - loss: 0.0691 - accuracy: 0.9771 - val_loss: 0.0268 - val_accuracy: 0.9910\n",
            "Epoch 14/20\n",
            "281/281 [==============================] - 78s 277ms/step - loss: 0.0629 - accuracy: 0.9796 - val_loss: 0.0244 - val_accuracy: 0.9920\n",
            "Epoch 15/20\n",
            "281/281 [==============================] - 78s 277ms/step - loss: 0.0484 - accuracy: 0.9849 - val_loss: 0.0280 - val_accuracy: 0.9910\n",
            "Epoch 16/20\n",
            "281/281 [==============================] - 78s 278ms/step - loss: 0.0453 - accuracy: 0.9872 - val_loss: 0.0287 - val_accuracy: 0.9920\n",
            "Epoch 17/20\n",
            "281/281 [==============================] - 78s 278ms/step - loss: 0.0380 - accuracy: 0.9876 - val_loss: 0.0209 - val_accuracy: 0.9940\n",
            "Epoch 18/20\n",
            "281/281 [==============================] - 78s 278ms/step - loss: 0.0417 - accuracy: 0.9883 - val_loss: 0.0237 - val_accuracy: 0.9925\n",
            "Epoch 19/20\n",
            "281/281 [==============================] - 78s 278ms/step - loss: 0.0357 - accuracy: 0.9899 - val_loss: 0.0188 - val_accuracy: 0.9935\n",
            "Epoch 20/20\n",
            "281/281 [==============================] - 78s 277ms/step - loss: 0.0368 - accuracy: 0.9888 - val_loss: 0.0253 - val_accuracy: 0.9900\n",
            "CNN Model 7: Epochs=20, Training accuracy=0.99041, Validation accuracy=0.99400\n",
            "Individual Net : 8\n",
            "Epoch 1/20\n",
            "281/281 [==============================] - 79s 277ms/step - loss: 1.4338 - accuracy: 0.5669 - val_loss: 1.8471 - val_accuracy: 0.4755\n",
            "Epoch 2/20\n",
            "281/281 [==============================] - 78s 279ms/step - loss: 0.2510 - accuracy: 0.9220 - val_loss: 0.0687 - val_accuracy: 0.9780\n",
            "Epoch 3/20\n",
            "281/281 [==============================] - 79s 280ms/step - loss: 0.1710 - accuracy: 0.9478 - val_loss: 0.0499 - val_accuracy: 0.9845\n",
            "Epoch 4/20\n",
            "281/281 [==============================] - 78s 278ms/step - loss: 0.1409 - accuracy: 0.9575 - val_loss: 0.0428 - val_accuracy: 0.9885\n",
            "Epoch 5/20\n",
            "281/281 [==============================] - 78s 277ms/step - loss: 0.1104 - accuracy: 0.9672 - val_loss: 0.0473 - val_accuracy: 0.9835\n",
            "Epoch 6/20\n",
            "281/281 [==============================] - 78s 279ms/step - loss: 0.0951 - accuracy: 0.9707 - val_loss: 0.0538 - val_accuracy: 0.9815\n",
            "Epoch 7/20\n",
            "281/281 [==============================] - 78s 278ms/step - loss: 0.0971 - accuracy: 0.9726 - val_loss: 0.0376 - val_accuracy: 0.9880\n",
            "Epoch 8/20\n",
            "281/281 [==============================] - 78s 277ms/step - loss: 0.0743 - accuracy: 0.9779 - val_loss: 0.0200 - val_accuracy: 0.9940\n",
            "Epoch 9/20\n",
            "281/281 [==============================] - 78s 278ms/step - loss: 0.0617 - accuracy: 0.9811 - val_loss: 0.0327 - val_accuracy: 0.9900\n",
            "Epoch 10/20\n",
            "281/281 [==============================] - 78s 278ms/step - loss: 0.0556 - accuracy: 0.9824 - val_loss: 0.0201 - val_accuracy: 0.9930\n",
            "Epoch 11/20\n",
            "281/281 [==============================] - 77s 275ms/step - loss: 0.0580 - accuracy: 0.9833 - val_loss: 0.0225 - val_accuracy: 0.9930\n",
            "Epoch 12/20\n",
            "281/281 [==============================] - 78s 276ms/step - loss: 0.0541 - accuracy: 0.9838 - val_loss: 0.0171 - val_accuracy: 0.9935\n",
            "Epoch 13/20\n",
            "281/281 [==============================] - 78s 278ms/step - loss: 0.0503 - accuracy: 0.9860 - val_loss: 0.0179 - val_accuracy: 0.9935\n",
            "Epoch 14/20\n",
            "281/281 [==============================] - 78s 277ms/step - loss: 0.0482 - accuracy: 0.9861 - val_loss: 0.0200 - val_accuracy: 0.9920\n",
            "Epoch 15/20\n",
            "281/281 [==============================] - 78s 279ms/step - loss: 0.0371 - accuracy: 0.9880 - val_loss: 0.0178 - val_accuracy: 0.9945\n",
            "Epoch 16/20\n",
            "281/281 [==============================] - 78s 279ms/step - loss: 0.0376 - accuracy: 0.9889 - val_loss: 0.0184 - val_accuracy: 0.9930\n",
            "Epoch 17/20\n",
            "281/281 [==============================] - 78s 278ms/step - loss: 0.0395 - accuracy: 0.9883 - val_loss: 0.0173 - val_accuracy: 0.9955\n",
            "Epoch 18/20\n",
            "281/281 [==============================] - 78s 279ms/step - loss: 0.0408 - accuracy: 0.9871 - val_loss: 0.0177 - val_accuracy: 0.9940\n",
            "Epoch 19/20\n",
            "281/281 [==============================] - 78s 279ms/step - loss: 0.0371 - accuracy: 0.9891 - val_loss: 0.0202 - val_accuracy: 0.9940\n",
            "Epoch 20/20\n",
            "281/281 [==============================] - 78s 279ms/step - loss: 0.0435 - accuracy: 0.9867 - val_loss: 0.0176 - val_accuracy: 0.9945\n",
            "CNN Model 8: Epochs=20, Training accuracy=0.98890, Validation accuracy=0.99550\n",
            "Individual Net : 9\n",
            "Epoch 1/20\n",
            "281/281 [==============================] - 79s 278ms/step - loss: 1.3764 - accuracy: 0.5868 - val_loss: 3.5986 - val_accuracy: 0.3425\n",
            "Epoch 2/20\n",
            "281/281 [==============================] - 78s 279ms/step - loss: 0.2548 - accuracy: 0.9178 - val_loss: 0.1094 - val_accuracy: 0.9660\n",
            "Epoch 3/20\n",
            "281/281 [==============================] - 78s 278ms/step - loss: 0.1776 - accuracy: 0.9439 - val_loss: 0.0643 - val_accuracy: 0.9815\n",
            "Epoch 4/20\n",
            "281/281 [==============================] - 78s 278ms/step - loss: 0.1350 - accuracy: 0.9595 - val_loss: 0.0521 - val_accuracy: 0.9830\n",
            "Epoch 5/20\n",
            "281/281 [==============================] - 78s 278ms/step - loss: 0.1185 - accuracy: 0.9650 - val_loss: 0.0395 - val_accuracy: 0.9870\n",
            "Epoch 6/20\n",
            "281/281 [==============================] - 78s 279ms/step - loss: 0.1143 - accuracy: 0.9631 - val_loss: 0.0389 - val_accuracy: 0.9845\n",
            "Epoch 7/20\n",
            "281/281 [==============================] - 78s 279ms/step - loss: 0.0965 - accuracy: 0.9726 - val_loss: 0.0380 - val_accuracy: 0.9860\n",
            "Epoch 8/20\n",
            "281/281 [==============================] - 78s 278ms/step - loss: 0.0914 - accuracy: 0.9733 - val_loss: 0.0458 - val_accuracy: 0.9855\n",
            "Epoch 9/20\n",
            "281/281 [==============================] - 78s 278ms/step - loss: 0.0707 - accuracy: 0.9777 - val_loss: 0.0334 - val_accuracy: 0.9890\n",
            "Epoch 10/20\n",
            "281/281 [==============================] - 79s 281ms/step - loss: 0.0574 - accuracy: 0.9821 - val_loss: 0.0239 - val_accuracy: 0.9915\n",
            "Epoch 11/20\n",
            "281/281 [==============================] - 78s 279ms/step - loss: 0.0648 - accuracy: 0.9798 - val_loss: 0.0207 - val_accuracy: 0.9940\n",
            "Epoch 12/20\n",
            "281/281 [==============================] - 78s 278ms/step - loss: 0.0571 - accuracy: 0.9830 - val_loss: 0.0269 - val_accuracy: 0.9920\n",
            "Epoch 13/20\n",
            "281/281 [==============================] - 78s 279ms/step - loss: 0.0591 - accuracy: 0.9817 - val_loss: 0.0241 - val_accuracy: 0.9920\n",
            "Epoch 14/20\n",
            "281/281 [==============================] - 79s 279ms/step - loss: 0.0569 - accuracy: 0.9834 - val_loss: 0.0205 - val_accuracy: 0.9925\n",
            "Epoch 15/20\n",
            "281/281 [==============================] - 79s 280ms/step - loss: 0.0484 - accuracy: 0.9850 - val_loss: 0.0196 - val_accuracy: 0.9930\n",
            "Epoch 16/20\n",
            "281/281 [==============================] - 79s 280ms/step - loss: 0.0436 - accuracy: 0.9872 - val_loss: 0.0195 - val_accuracy: 0.9940\n",
            "Epoch 17/20\n",
            "281/281 [==============================] - 79s 280ms/step - loss: 0.0452 - accuracy: 0.9859 - val_loss: 0.0175 - val_accuracy: 0.9935\n",
            "Epoch 18/20\n",
            "281/281 [==============================] - 79s 279ms/step - loss: 0.0423 - accuracy: 0.9870 - val_loss: 0.0147 - val_accuracy: 0.9955\n",
            "Epoch 19/20\n",
            "281/281 [==============================] - 78s 279ms/step - loss: 0.0316 - accuracy: 0.9897 - val_loss: 0.0167 - val_accuracy: 0.9945\n",
            "Epoch 20/20\n",
            "281/281 [==============================] - 78s 278ms/step - loss: 0.0359 - accuracy: 0.9883 - val_loss: 0.0169 - val_accuracy: 0.9935\n",
            "CNN Model 9: Epochs=20, Training accuracy=0.98890, Validation accuracy=0.99550\n",
            "Individual Net : 10\n",
            "Epoch 1/20\n",
            "281/281 [==============================] - 79s 278ms/step - loss: 1.3731 - accuracy: 0.5871 - val_loss: 2.1934 - val_accuracy: 0.3985\n",
            "Epoch 2/20\n",
            "281/281 [==============================] - 78s 278ms/step - loss: 0.2427 - accuracy: 0.9222 - val_loss: 0.1812 - val_accuracy: 0.9390\n",
            "Epoch 3/20\n",
            "281/281 [==============================] - 78s 279ms/step - loss: 0.1577 - accuracy: 0.9507 - val_loss: 0.0641 - val_accuracy: 0.9800\n",
            "Epoch 4/20\n",
            "281/281 [==============================] - 79s 280ms/step - loss: 0.1343 - accuracy: 0.9593 - val_loss: 0.0529 - val_accuracy: 0.9880\n",
            "Epoch 5/20\n",
            "281/281 [==============================] - 79s 280ms/step - loss: 0.1123 - accuracy: 0.9667 - val_loss: 0.0480 - val_accuracy: 0.9875\n",
            "Epoch 6/20\n",
            "281/281 [==============================] - 79s 281ms/step - loss: 0.1003 - accuracy: 0.9677 - val_loss: 0.0528 - val_accuracy: 0.9855\n",
            "Epoch 7/20\n",
            "281/281 [==============================] - 79s 279ms/step - loss: 0.0999 - accuracy: 0.9704 - val_loss: 0.0544 - val_accuracy: 0.9865\n",
            "Epoch 8/20\n",
            "281/281 [==============================] - 78s 279ms/step - loss: 0.0693 - accuracy: 0.9767 - val_loss: 0.0440 - val_accuracy: 0.9880\n",
            "Epoch 9/20\n",
            "281/281 [==============================] - 79s 283ms/step - loss: 0.0693 - accuracy: 0.9796 - val_loss: 0.0359 - val_accuracy: 0.9915\n",
            "Epoch 10/20\n",
            "281/281 [==============================] - 79s 282ms/step - loss: 0.0558 - accuracy: 0.9828 - val_loss: 0.0449 - val_accuracy: 0.9885\n",
            "Epoch 11/20\n",
            "281/281 [==============================] - 79s 280ms/step - loss: 0.0591 - accuracy: 0.9820 - val_loss: 0.0284 - val_accuracy: 0.9915\n",
            "Epoch 12/20\n",
            "281/281 [==============================] - 79s 279ms/step - loss: 0.0572 - accuracy: 0.9837 - val_loss: 0.0405 - val_accuracy: 0.9890\n",
            "Epoch 13/20\n",
            "281/281 [==============================] - 79s 279ms/step - loss: 0.0444 - accuracy: 0.9863 - val_loss: 0.0286 - val_accuracy: 0.9925\n",
            "Epoch 14/20\n",
            "281/281 [==============================] - 78s 279ms/step - loss: 0.0439 - accuracy: 0.9858 - val_loss: 0.0322 - val_accuracy: 0.9920\n",
            "Epoch 15/20\n",
            "281/281 [==============================] - 79s 279ms/step - loss: 0.0402 - accuracy: 0.9874 - val_loss: 0.0312 - val_accuracy: 0.9920\n",
            "Epoch 16/20\n",
            "281/281 [==============================] - 78s 278ms/step - loss: 0.0439 - accuracy: 0.9863 - val_loss: 0.0279 - val_accuracy: 0.9930\n",
            "Epoch 17/20\n",
            "281/281 [==============================] - 78s 276ms/step - loss: 0.0392 - accuracy: 0.9878 - val_loss: 0.0265 - val_accuracy: 0.9940\n",
            "Epoch 18/20\n",
            "281/281 [==============================] - 78s 278ms/step - loss: 0.0437 - accuracy: 0.9873 - val_loss: 0.0361 - val_accuracy: 0.9900\n",
            "Epoch 19/20\n",
            "281/281 [==============================] - 78s 278ms/step - loss: 0.0444 - accuracy: 0.9865 - val_loss: 0.0299 - val_accuracy: 0.9905\n",
            "Epoch 20/20\n",
            "281/281 [==============================] - 78s 279ms/step - loss: 0.0349 - accuracy: 0.9884 - val_loss: 0.0301 - val_accuracy: 0.9920\n",
            "CNN Model 10: Epochs=20, Training accuracy=0.98857, Validation accuracy=0.99400\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XY8eDDpP-M5X"
      },
      "source": [
        "#Result\n",
        "results = np.zeros( (test.shape[0],10) ) \n",
        "for j in range(nets):\n",
        "    results = results + model[j].predict(test)\n",
        " \n",
        "results = np.argmax(results,axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-cgYjqQ-O4-"
      },
      "source": [
        "#Test on result\n",
        "plt.imshow(test[0][:,:,0])\n",
        "plt.title(results[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sExuVuOa-RP8"
      },
      "source": [
        "L = 4\n",
        "W = 4\n",
        "fig, axes = plt.subplots(L, W, figsize = (12,12))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for i in np.arange(0, L * W):  \n",
        "    axes[i].imshow(test[i].reshape(28,28))\n",
        "    axes[i].set_title(results[i])\n",
        "    axes[i].axis('off')\n",
        "plt.subplots_adjust(wspace=0.5)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}